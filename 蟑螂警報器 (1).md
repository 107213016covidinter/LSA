# 蟑螂警報器
## 動機
住在研宿時，常常都需要注意門縫會不會鑽入不速之客，也就是令人畏懼的小強...
所以我們希望能設計一個能隨時幫我盯著門口，一發現小強立刻進行驅趕，以達到世界和平的目的。
## 需準備器材
webcam x1 
樹梅派  x1 
蟑螂本人或蟑螂圖片 x1 
馬達 x1
沉水馬達+水管 x1
#### 升級環境
```
sudo apt-get update
```
```
sudo apt-get upgrade
```
### 環境設置
#### 安裝tensorFlowlite
下載安裝包
```
git clone https://github.com/EDjeElectronics/TensorFlow-Lite-Object-Dection-on-Android-and-RaspberryPi.git
```
將安裝包移到tflite1下
```
mv TensorFlow-Lite-Object-Dection-on-Android-and-RaspberryPi /tflite1
```
切換到tflite1下
```
cd tflite1
```
安裝virtualenv
```
sudo pip3 install virtualenv
```
創建虛擬環境
```
python3-m venv tflite1-env
```
進入虛擬環境
```
source tflite1-env/bin/activate
```
安裝偵測所需要的工具
```
bash get_pi_requirements.sh
```
安裝ubuchtl(控制usb)
```
sudo apt install ubuchtl
```
安裝RPio.gpio(控制馬達)
```
pip3 install RPio.GPIO
```
下載測試模型
```
wget https://storage.googleapis.com/download.tensorflow.org/models/tflifte/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip
```
解壓縮安裝包
```
unzip coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip -d Sample_TFLite_model
```
測試環境
```
python3 TFLite_detection_webcam.py --modeldir=Sample_TFLite_model
```



#### 創建model
[google teachable machine](https://teachablemachine.withgoogle.com/)
按Get Started
![](https://i.imgur.com/B3qkWa7.png)
選擇image Project
![](https://i.imgur.com/8vvKRNW.png)
選擇standard image model
![](https://i.imgur.com/RriFQYa.png)
輸入資料集，這邊我們使用Kaggle上的[資料集](https://www.kaggle.com/mathewribeiro/spiderscorpioncochroach)
![](https://i.imgur.com/kiIgr3B.png)
![](https://i.imgur.com/MMCj1p0.png)
訓練模型
![](https://i.imgur.com/0Q54n5v.png)
下載模型
![](https://i.imgur.com/APNK4GI.png)
#### 在樹梅派上執行model
將model放入資料夾中
![](https://i.imgur.com/MjtfuAh.png)
創建執行程式
```
vim TM2_tflite.py
```
程式碼
```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import argparse
import io
import os
import time
import numpy as np
#import picamera
import cv2

#control engine
import RPi.GPIO as GPIO  # sudo apt-get install rpi.gpio

CONTROL_PIN = 17
PWM_FREQ = 50
STEP = 15

GPIO.setmode(GPIO.BCM)
GPIO.setup(CONTROL_PIN,GPIO.OUT)

pwm = GPIO.PWM(CONTROL_PIN,PWM_FREQ)
pwm.start(10)

def angle(angle = 0):
    duty_cycle = (0.05 * PWM_FREQ) + (0.19 * PWM_FREQ * angle / 180)
    return duty_cycle
def ss(deg):
    dc = angle(deg)
    pwm.ChangeDutyCycle(dc)
    


#import tensorflow as tf

from PIL import Image
from tflite_runtime.interpreter import Interpreter

def load_labels(path):
  with open(path, 'r') as f:
    return {i: line.strip() for i, line in enumerate(f.readlines())}


def set_input_tensor(interpreter, image):
  tensor_index = interpreter.get_input_details()[0]['index']
  input_tensor = interpreter.tensor(tensor_index)()[0]
  input_tensor[:, :] = image


def classify_image(interpreter, image, top_k=1):
  """Returns a sorted array of classification results."""
  set_input_tensor(interpreter, image)
  interpreter.invoke()
  output_details = interpreter.get_output_details()[0]
  output = np.squeeze(interpreter.get_tensor(output_details['index']))

  # If the model is quantized (uint8 data), then dequantize the results
  if output_details['dtype'] == np.uint8:
    scale, zero_point = output_details['quantization']
    output = scale * (output - zero_point)

  ordered = np.argpartition(-output, top_k)
  return [(i, output[i]) for i in ordered[:top_k]]

def control_engine():
  os.system('cd ../../uhubctl;sudo ./uhubctl -l 1-1 -p 5 -a on')
  change = [0,180]
  for i in range(2):
    for j in range(2):
      ss(change[j])
      time.sleep(2)
  os.system('cd ../../uhubctl;sudo ./uhubctl -l 1-1 -p 5 -a off')
  
  

def main():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument(
      '--model', help='File path of .tflite file.', required=True)
  parser.add_argument(
      '--labels', help='File path of labels file.', required=True)
  args = parser.parse_args()

  labels = load_labels(args.labels)

  #interpreter = tf.lite.Interpreter(args.model)
  interpreter = Interpreter(args.model)

  interpreter.allocate_tensors()
  _, height, width, _ = interpreter.get_input_details()[0]['shape']

  #with picamera.PiCamera(resolution=(640, 480), framerate=30) as camera:
    #camera.start_preview()

  cap = cv2.VideoCapture(0)
  #擷取畫面 寬度 設定為640
  cap.set(cv2.CAP_PROP_FRAME_WIDTH,640)
  #擷取畫面 高度 設定為480
  cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

  key_detect = 0
  times=1
  while (key_detect==0):
    ret,image_src =cap.read()

    frame_width=image_src.shape[1]
    frame_height=image_src.shape[0]

    cut_d=int((frame_width-frame_height)/2)
    crop_img=image_src[0:frame_height,cut_d:(cut_d+frame_height)]

    image=cv2.resize(crop_img,(224,224),interpolation=cv2.INTER_AREA)

    start_time = time.time()
    if (times==1):
      results = classify_image(interpreter, image)
      elapsed_ms = (time.time() - start_time) * 1000
      label_id, prob = results[0]

      print(labels[label_id],prob)
      #retrun value
      if (label_id == 0):
        control_engine()
        key_detect = 1
        
      
    cv2.putText(crop_img,labels[label_id] + " " + str(round(prob,3)), (5,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 1, cv2.LINE_AA)

    times=times+1
    if (times>1):
      times=1

    cv2.imshow('Detecting....',crop_img)

    if cv2.waitKey(1) & 0xFF == ord('q'):
      key_detect = 1

  cap.release()
  cv2.destroyAllWindows()
  

if __name__ == '__main__':
  #os.system('cd ../../uhubctl;sudo ./uhubctl -l 1-1 -p 5 -a off')
  while True:
      main()
      time.sleep(4)
```
執行
```
python3 TM2_tflite.py --model model.tflite --labels labels.txt
```
執行畫面
![](https://i.imgur.com/lSMTFeV.png)
![](https://i.imgur.com/mVGNzhF.png)
### 分工表


| 姓名 | 做的事 |
| -------- | -------- |
| 施凱翔    | 程式整合、debug     |
|  嚴凱     |  架設環境、訓練模型     |
| 江柏諺     | 撰寫github、查資料     |
| 吳健瑋     |購買材料、查資料     |

### 特別感謝
在此特別感謝以下夥伴友情贊助:
感謝 李漢偉、蔣毓婷 助教提供方法意見
張華哲 : 無線鍵盤組、延長線、隨身碟、餅乾
歐哲安 : 螢幕、VGA轉HTMI線

### 參考資料
[環境設置教學影片](https://www.youtube.com/watch?v=aimSGOAUI8Y)
[google teachable machine教學](https://www.rs-online.com/designspark/google-teachable-machine-raspberry-pi-4-cn)


